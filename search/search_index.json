{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A SciCat production environment optimized for small scale installations DAPHNE4NFDI This project is initiated within the framework of DAPHNE4NFDI , the \"Data for Photon and Neutron Science\" consortium within NFDI . With this project we aim to provide a easy to use SciCat environment tailored to the needs of university research groups and labs that would like to run small SciCat instances. We also try to provide ideas and drafts of potential extensions to SciCat especially useful in these environments. Once a useful set of additional SciCat features has been agreed on within DAPHNE they will be put forward be integrated into the SciCat project. For this reason we work in close collaborations with the core developers of SciCat. We aim to coordinate collaboratively the SciCat development activities within DAPHNE to ensure its sustainability. The boundary conditions put to this project are the solution should be usable by IT-informed scientist that are use to Linux environments. it should run on a single (virtual) machine (e.g. 2 GB RAM, 2 cores) Contributions welcome! If you consider to participate or to continue to develop this project have a look at SciCat4daphne on GitHub Scicat Schema Extension and contat alexander.hinderhofer [at] uni-tuebingen.de or linus.pithan [at] desy.de. Extensions to SciCat within this package there are currently two additional features extending SciCat's current capabilities: A web-service to create new Datasets through a GUI A metadata schema management and validation tool (frontend + backend) for Scientific Metadata API endpoints without authentication While the GUI to create Datasets through a GUI is using the user authentication of SciCat there are some additional API endpoints for schema management that work without user authentication. For this reason the Scicat Schema Extension is rather to be considered to be in a proof of concept state than being production ready. scicatlive vs. scicat4daphne The solution provided in this package is based on scicatlive which rather addresses the needs of software developers. We have worked on it to adopt it to our needs. The main difference are: use of a persistent mongodb in the backend configurable frontend using prebuild docker images integration of additional services","title":"A SciCat production environment optimized for small scale installations"},{"location":"#a-scicat-production-environment-optimized-for-small-scale-installations","text":"DAPHNE4NFDI This project is initiated within the framework of DAPHNE4NFDI , the \"Data for Photon and Neutron Science\" consortium within NFDI . With this project we aim to provide a easy to use SciCat environment tailored to the needs of university research groups and labs that would like to run small SciCat instances. We also try to provide ideas and drafts of potential extensions to SciCat especially useful in these environments. Once a useful set of additional SciCat features has been agreed on within DAPHNE they will be put forward be integrated into the SciCat project. For this reason we work in close collaborations with the core developers of SciCat. We aim to coordinate collaboratively the SciCat development activities within DAPHNE to ensure its sustainability. The boundary conditions put to this project are the solution should be usable by IT-informed scientist that are use to Linux environments. it should run on a single (virtual) machine (e.g. 2 GB RAM, 2 cores) Contributions welcome! If you consider to participate or to continue to develop this project have a look at SciCat4daphne on GitHub Scicat Schema Extension and contat alexander.hinderhofer [at] uni-tuebingen.de or linus.pithan [at] desy.de.","title":"A SciCat production environment optimized for small scale installations"},{"location":"#extensions-to-scicat","text":"within this package there are currently two additional features extending SciCat's current capabilities: A web-service to create new Datasets through a GUI A metadata schema management and validation tool (frontend + backend) for Scientific Metadata API endpoints without authentication While the GUI to create Datasets through a GUI is using the user authentication of SciCat there are some additional API endpoints for schema management that work without user authentication. For this reason the Scicat Schema Extension is rather to be considered to be in a proof of concept state than being production ready.","title":"Extensions to SciCat"},{"location":"#scicatlive-vs-scicat4daphne","text":"The solution provided in this package is based on scicatlive which rather addresses the needs of software developers. We have worked on it to adopt it to our needs. The main difference are: use of a persistent mongodb in the backend configurable frontend using prebuild docker images integration of additional services","title":"scicatlive vs. scicat4daphne"},{"location":"access_managment/","text":"Authentication This is still an open issue. So far, for small installations, only the use of so called functional accounts is supported. The user credentials are to be specified in scicatlive4daphne/config/backend/functionalAccounts.json and are imported from here into the underlying MongoDB . This process is triggered on each start or restart of the backend service. Once imported, the account credentials are no longer needed in functionalAccounts.json and should be removed from this file to avoid security threats.","title":"Access managment"},{"location":"access_managment/#authentication","text":"This is still an open issue. So far, for small installations, only the use of so called functional accounts is supported. The user credentials are to be specified in scicatlive4daphne/config/backend/functionalAccounts.json and are imported from here into the underlying MongoDB . This process is triggered on each start or restart of the backend service. Once imported, the account credentials are no longer needed in functionalAccounts.json and should be removed from this file to avoid security threats.","title":"Authentication"},{"location":"ingestion_frontend/","text":"Upload / ingestion frontend With this additional service we aim to provide the possibility create Datasets with structured Metadata through a web frontend. In the given configuration this service that is part of the Scicat Schema Extension is accessible at http://scicat-host/upload .","title":"Upload / ingestion frontend"},{"location":"ingestion_frontend/#upload-ingestion-frontend","text":"With this additional service we aim to provide the possibility create Datasets with structured Metadata through a web frontend. In the given configuration this service that is part of the Scicat Schema Extension is accessible at http://scicat-host/upload .","title":"Upload / ingestion frontend"},{"location":"installation/","text":"Project is still under active development Please consider this project in it's current state just as a draft. The api is not yet stable and there may still be important security vulnerabilities. Documentation is still far from being complete. Installation of SciCat4daphne Requirements The described installation has been tested on the following systems: Virtual machine running Ubuntu 20.04 Tested dependency Versions: docker version 24 docker-compose version 1.29, installed via pip Getting started Just navigate into the scicatlive4daphne directory and run docker-compose up -d to look at log messages use docker-compose logs to stop all services use docker-compose down Necessary adoptions configuration of upload frontend password of MongoExpress passwords of default users configuration templates are provided in scicatlive4daphne/config Services in detail This is an overview of the different services invoked through docker-compose together with the most important configuration options. SciCat Backend the configuration file is located in /scicat4daphne/scicatlive4daphne/config/backend/config.env but should not need site-specific changes for a basic installation. For a production environment e.g. the entries pidPrefix and site should be specified. SciCat Frontend this service uses the configuration file scicatlive4daphne/config/frontend/config.json where e.g. the appearance of the scientific metadata (tree view vs. simple tabular view) and the columns of the dataset table can be configured. Reverse Proxy (Traefik) this service is used to stitch the different services in docker-compose together Database (MongoDB) all configuration happens in the docker-compose.yaml . Most important is the mounting point for the _ mongoDB_ on the host system which is specified at volumes: - \"/srv/mongodb:/bitnami/mongodb\" # check that is mount exists on local file-system in the docker-compose.yaml there are also some hints for database backups using the provided script. Upload Frontend Can be found at http://scicat-host/upload with configuration in scicatlive4daphne/config/upload/env.js . This file needs to be adopted for each running SciCat instance. window.env = { \"REACT_APP_API_URL\": \"http://your-scicat-domain.org/api/v3\", \"REACT_APP_API_URL2\": \"http://your-scicat-domain.org\", \"REACT_APP_STORE_KEY\": \"scicat\", \"REACT_APP_EXTERNAL_DATASETS_URL\": \"http://your-scicat-domain.org/datasets\", \"REACT_APP_ROUTER_BASENAME\": \"/upload\", \"PUBLIC_URL\": \"http://your-scicat-domain.org/upload\", \"REACT_APP_SCICAT_DEFAULT_DS_OWNER\": \"a_owner\", \"REACT_APP_SCICAT_DEFAULT_DS_GROUP\": \"a_group\", \"REACT_APP_SCICAT_DEFAULT_DS_OWNER_GROUP\": \"a_owner_group\" } the primary change needed is to exchange your-scicat-domain.org according to the local conditions. For test installation e.g. replace with localhost Addon API Currently there is no configuration for this service. An interactive api-documentation can be found at http://scicat-host/swagger-ui . Not Secure calls to this additional backend api currently do not rely on any authentication MongoDB Web GUI (MongoExpress) Mainly intended for those who are managing the installation and for inspection of the DB. By default this service is deactivated (commented out in docker-compose.yaml ). The login for this service is specified in the docker-compose.yaml and managed by traefik . The password is hashed using \ud83d\udd34 which can e.g. be generated at \ud83d\udd34. Maintenance and House Keeping Backup The current backup strategy is to run the script scicatlive4daphne\\backup.sh periodically e.g. in a cron job running it via docker inside an already running container via docker exec scicatlive4daphne_mongodb_1 /backup.sh from the host system. This script basically relies on mongodump . Restore from backup Please have a look at the documentation of mongorestore . In case you want to import an existing dump please do so starting from a clean Mongo DB before starting the SciCat backend for the first time.","title":"Installation"},{"location":"installation/#installation-of-scicat4daphne","text":"","title":"Installation of SciCat4daphne"},{"location":"installation/#requirements","text":"The described installation has been tested on the following systems: Virtual machine running Ubuntu 20.04 Tested dependency Versions: docker version 24 docker-compose version 1.29, installed via pip","title":"Requirements"},{"location":"installation/#getting-started","text":"Just navigate into the scicatlive4daphne directory and run docker-compose up -d to look at log messages use docker-compose logs to stop all services use docker-compose down","title":"Getting started"},{"location":"installation/#necessary-adoptions","text":"configuration of upload frontend password of MongoExpress passwords of default users configuration templates are provided in scicatlive4daphne/config","title":"Necessary adoptions"},{"location":"installation/#services-in-detail","text":"This is an overview of the different services invoked through docker-compose together with the most important configuration options.","title":"Services in detail"},{"location":"installation/#scicat-backend","text":"the configuration file is located in /scicat4daphne/scicatlive4daphne/config/backend/config.env but should not need site-specific changes for a basic installation. For a production environment e.g. the entries pidPrefix and site should be specified.","title":"SciCat Backend"},{"location":"installation/#scicat-frontend","text":"this service uses the configuration file scicatlive4daphne/config/frontend/config.json where e.g. the appearance of the scientific metadata (tree view vs. simple tabular view) and the columns of the dataset table can be configured.","title":"SciCat Frontend"},{"location":"installation/#reverse-proxy-traefik","text":"this service is used to stitch the different services in docker-compose together","title":"Reverse Proxy (Traefik)"},{"location":"installation/#database-mongodb","text":"all configuration happens in the docker-compose.yaml . Most important is the mounting point for the _ mongoDB_ on the host system which is specified at volumes: - \"/srv/mongodb:/bitnami/mongodb\" # check that is mount exists on local file-system in the docker-compose.yaml there are also some hints for database backups using the provided script.","title":"Database (MongoDB)"},{"location":"installation/#upload-frontend","text":"Can be found at http://scicat-host/upload with configuration in scicatlive4daphne/config/upload/env.js . This file needs to be adopted for each running SciCat instance. window.env = { \"REACT_APP_API_URL\": \"http://your-scicat-domain.org/api/v3\", \"REACT_APP_API_URL2\": \"http://your-scicat-domain.org\", \"REACT_APP_STORE_KEY\": \"scicat\", \"REACT_APP_EXTERNAL_DATASETS_URL\": \"http://your-scicat-domain.org/datasets\", \"REACT_APP_ROUTER_BASENAME\": \"/upload\", \"PUBLIC_URL\": \"http://your-scicat-domain.org/upload\", \"REACT_APP_SCICAT_DEFAULT_DS_OWNER\": \"a_owner\", \"REACT_APP_SCICAT_DEFAULT_DS_GROUP\": \"a_group\", \"REACT_APP_SCICAT_DEFAULT_DS_OWNER_GROUP\": \"a_owner_group\" } the primary change needed is to exchange your-scicat-domain.org according to the local conditions. For test installation e.g. replace with localhost","title":"Upload Frontend"},{"location":"installation/#addon-api","text":"Currently there is no configuration for this service. An interactive api-documentation can be found at http://scicat-host/swagger-ui . Not Secure calls to this additional backend api currently do not rely on any authentication","title":"Addon API"},{"location":"installation/#mongodb-web-gui-mongoexpress","text":"Mainly intended for those who are managing the installation and for inspection of the DB. By default this service is deactivated (commented out in docker-compose.yaml ). The login for this service is specified in the docker-compose.yaml and managed by traefik . The password is hashed using \ud83d\udd34 which can e.g. be generated at \ud83d\udd34.","title":"MongoDB Web GUI (MongoExpress)"},{"location":"installation/#maintenance-and-house-keeping","text":"","title":"Maintenance and House Keeping"},{"location":"installation/#backup","text":"The current backup strategy is to run the script scicatlive4daphne\\backup.sh periodically e.g. in a cron job running it via docker inside an already running container via docker exec scicatlive4daphne_mongodb_1 /backup.sh from the host system. This script basically relies on mongodump .","title":"Backup"},{"location":"installation/#restore-from-backup","text":"Please have a look at the documentation of mongorestore . In case you want to import an existing dump please do so starting from a clean Mongo DB before starting the SciCat backend for the first time.","title":"Restore from backup"},{"location":"metadata_managment/","text":"Metadata Schemas One of the unique features of SciCat is its flexibility in capturing Scientific Metadata. However, to assure a systematic (machine readable and AI-ready) structure of the metadata captured in SciCat one approach is to use well defined Metadata Schemas . Ideally these schemata are predefined and follow given ontologies (e.g. the PaNdata Ontology) or rules that are accepted in the community (e.g. NeXus). Unfortunately for many use cases it is very difficult to agree on a ridged metadata structure beforehand. Here we try to follow a more flexible approach that explicitly allows the creation and modification of metadata schemas any time. This is rather a pragmatic decision mainly due to the fact that for many experimental datasets it is not yet clear which metadata should be captured and to some extend it just postpones the decision on the final, ridged metadata structure. In a way this approach is inspired by what was developed in AMORE / AMARCORD (EuXFEL) . The SciCat Extention proposed here offers the following features: Metadata Schemas are stored in the mongoDB of SciCat Creation and Management of Metadata Schemas through the web frontend (see ingestion frontend) Metadata Schemas can be specific for \"Datasets\" or \"Samples\" Validation to check if of provided metadata follows the available metadata schemas Support of schemas with a managed catalog of entries Schema specification Schema structure { \"schema_name\": \"measurement\", \"schema_type\": \"dataset\" \"fixed_value_entries\": false, \"keys\": [ { \"key_name\": \"measurement_type\", \"type\": \"string\", \"required\": true, \"unit\": null \"allowed\": [\"beamtime\", \"lab\"], \"changes_likely\": false, \"scan_ref\": false, } ], } The basic fields in the schema are schema_name and schema_type , where the type can either be dataset or sample . fixed_value_entries specifies \ud83d\udd36. For each key in the schema there are the following options: key_name : field name type : valid values are boolean , string , number , float , int , list required : specifies if a value for this field must be provided in order to pass validation unit : if there is a unit attached to this field it can be provided here allowed : can be used to specify a list of allowed values that will be changes_likely : highlights fields that are likely to change in case an exiting dataset is used as template for a new one. scan_ref : ... not yet used. Intended for validation in case of data references (might be removed in the future) Examples of Metadata schemas Metadata schemas dedicated to a experimental techniques { \"schema_name\": \"GIWAXS\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": true, \"key_name\": \"sample_detector_distance\", \"required\": true, \"scan_ref\": false, \"type\": \"number\", \"unit\": \"mm\" }, { \"changes_likely\": true, \"key_name\": \"central_pixel\", \"required\": true, \"scan_ref\": false, \"schema\": { \"type\": \"number\" }, \"type\": \"list\", \"unit\": null } ] } Metadata schemas describing the dataset structure / experimental procedures { \"schema_name\": \"logbook\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": false, \"key_name\": \"logbook_file\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"logbook_pages\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] } Managed schemas / Auto-complete schemas The idea here is to have not only the schema in the DB but also a list of know entries to these schemas that can be can be used as metadata. This is what the fixed_value_entries key refers to. Example: Material database for samples One use-case for this could be a sort of DB which contains materials samples can be composed of. Here is an example for such a schema: { \"schema_name\": \"material\", \"schema_type\": \"sample\", \"multiples_entries\": true, \"fixed_value_entries\": true, \"id_key\": \"material_id\", \"keys\": [ { \"changes_likely\": true, \"key_name\": \"material_id\", \"required\": true, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"full_name\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"formula\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] } and some entries in corresponding database collections would be: { material_id: 'PEN', full_name: 'Pentacene', formula: 'C22H14' }, { material_id: 'Au', full_name: 'gold' }","title":"Metadata Schemas"},{"location":"metadata_managment/#metadata-schemas","text":"One of the unique features of SciCat is its flexibility in capturing Scientific Metadata. However, to assure a systematic (machine readable and AI-ready) structure of the metadata captured in SciCat one approach is to use well defined Metadata Schemas . Ideally these schemata are predefined and follow given ontologies (e.g. the PaNdata Ontology) or rules that are accepted in the community (e.g. NeXus). Unfortunately for many use cases it is very difficult to agree on a ridged metadata structure beforehand. Here we try to follow a more flexible approach that explicitly allows the creation and modification of metadata schemas any time. This is rather a pragmatic decision mainly due to the fact that for many experimental datasets it is not yet clear which metadata should be captured and to some extend it just postpones the decision on the final, ridged metadata structure. In a way this approach is inspired by what was developed in AMORE / AMARCORD (EuXFEL) . The SciCat Extention proposed here offers the following features: Metadata Schemas are stored in the mongoDB of SciCat Creation and Management of Metadata Schemas through the web frontend (see ingestion frontend) Metadata Schemas can be specific for \"Datasets\" or \"Samples\" Validation to check if of provided metadata follows the available metadata schemas Support of schemas with a managed catalog of entries","title":"Metadata Schemas"},{"location":"metadata_managment/#schema-specification","text":"","title":"Schema specification"},{"location":"metadata_managment/#schema-structure","text":"{ \"schema_name\": \"measurement\", \"schema_type\": \"dataset\" \"fixed_value_entries\": false, \"keys\": [ { \"key_name\": \"measurement_type\", \"type\": \"string\", \"required\": true, \"unit\": null \"allowed\": [\"beamtime\", \"lab\"], \"changes_likely\": false, \"scan_ref\": false, } ], } The basic fields in the schema are schema_name and schema_type , where the type can either be dataset or sample . fixed_value_entries specifies \ud83d\udd36. For each key in the schema there are the following options: key_name : field name type : valid values are boolean , string , number , float , int , list required : specifies if a value for this field must be provided in order to pass validation unit : if there is a unit attached to this field it can be provided here allowed : can be used to specify a list of allowed values that will be changes_likely : highlights fields that are likely to change in case an exiting dataset is used as template for a new one. scan_ref : ... not yet used. Intended for validation in case of data references (might be removed in the future)","title":"Schema structure"},{"location":"metadata_managment/#examples-of-metadata-schemas","text":"","title":"Examples of Metadata schemas"},{"location":"metadata_managment/#metadata-schemas-dedicated-to-a-experimental-techniques","text":"{ \"schema_name\": \"GIWAXS\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": true, \"key_name\": \"sample_detector_distance\", \"required\": true, \"scan_ref\": false, \"type\": \"number\", \"unit\": \"mm\" }, { \"changes_likely\": true, \"key_name\": \"central_pixel\", \"required\": true, \"scan_ref\": false, \"schema\": { \"type\": \"number\" }, \"type\": \"list\", \"unit\": null } ] }","title":"Metadata schemas dedicated to a experimental techniques"},{"location":"metadata_managment/#metadata-schemas-describing-the-dataset-structure-experimental-procedures","text":"{ \"schema_name\": \"logbook\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": false, \"key_name\": \"logbook_file\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"logbook_pages\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] }","title":"Metadata schemas describing the dataset structure / experimental procedures"},{"location":"metadata_managment/#managed-schemas-auto-complete-schemas","text":"The idea here is to have not only the schema in the DB but also a list of know entries to these schemas that can be can be used as metadata. This is what the fixed_value_entries key refers to.","title":"Managed schemas / Auto-complete schemas"},{"location":"metadata_managment/#example-material-database-for-samples","text":"One use-case for this could be a sort of DB which contains materials samples can be composed of. Here is an example for such a schema: { \"schema_name\": \"material\", \"schema_type\": \"sample\", \"multiples_entries\": true, \"fixed_value_entries\": true, \"id_key\": \"material_id\", \"keys\": [ { \"changes_likely\": true, \"key_name\": \"material_id\", \"required\": true, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"full_name\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"formula\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] } and some entries in corresponding database collections would be: { material_id: 'PEN', full_name: 'Pentacene', formula: 'C22H14' }, { material_id: 'Au', full_name: 'gold' }","title":"Example: Material database for samples"}]}