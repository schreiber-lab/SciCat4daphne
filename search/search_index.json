{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project is still under active development Please consider this project in it's current state just as a draft. The api is not yet stable and there may still be important security vulnerabilities. Documentation is still far from being complete. A SciCat production environment optimized for small scale installations DAPHNE4NFDI This project is initiated within the framework of DAPHNE4NFDI , the \"Data for Photon and Neutron Science\" consortium within NFDI . With this project we aim to provide a easy to use SciCat environment tailored to the needs of university research groups and labs that would like to run small SciCat instances. We also try to provide ideas and drafts of potential extensions to SciCat especially useful in these environments. Once a useful set of additional SciCat features has been agreed on within DAPHNE they will be put forward be integrated into the SciCat project. For this reason we work in close collaborations with the core developers of SciCat. We aim to coordinate collaboratively the SciCat development activities within DAPHNE to ensure its sustainability. The boundary conditions put to this project are the solution should be usable by IT-informed scientist that are use to Linux environments. it should run on a single (virtual) machine (e.g. 2 GB RAM, 2 cores) Contributions welcome! Please consider to participate or ask questions using github issues or pull requests. SciCat4daphne on GitHub For further enquirers please contact linus.pithan [at] uni-tuebingen.de. Extensions to SciCat within this package there are currently two additional features extending SciCat's current capabilities: A web-service to create new Datasets through a GUI A metadata schema management and validation tool (frontend + backend) for Scientific Metadata scicatlive vs. scicat4daphne The solution provided in this package is based on scicatlive which rather addresses the needs of software developers. We have worked on it to adopt it to our needs. The main difference are: use of a persistent mongodb in the backend configurable frontend using prebuild docker images integration of additional services","title":"Home"},{"location":"#a-scicat-production-environment-optimized-for-small-scale-installations","text":"DAPHNE4NFDI This project is initiated within the framework of DAPHNE4NFDI , the \"Data for Photon and Neutron Science\" consortium within NFDI . With this project we aim to provide a easy to use SciCat environment tailored to the needs of university research groups and labs that would like to run small SciCat instances. We also try to provide ideas and drafts of potential extensions to SciCat especially useful in these environments. Once a useful set of additional SciCat features has been agreed on within DAPHNE they will be put forward be integrated into the SciCat project. For this reason we work in close collaborations with the core developers of SciCat. We aim to coordinate collaboratively the SciCat development activities within DAPHNE to ensure its sustainability. The boundary conditions put to this project are the solution should be usable by IT-informed scientist that are use to Linux environments. it should run on a single (virtual) machine (e.g. 2 GB RAM, 2 cores) Contributions welcome! Please consider to participate or ask questions using github issues or pull requests. SciCat4daphne on GitHub For further enquirers please contact linus.pithan [at] uni-tuebingen.de.","title":"A SciCat production environment optimized for small scale installations"},{"location":"#extensions-to-scicat","text":"within this package there are currently two additional features extending SciCat's current capabilities: A web-service to create new Datasets through a GUI A metadata schema management and validation tool (frontend + backend) for Scientific Metadata","title":"Extensions to SciCat"},{"location":"#scicatlive-vs-scicat4daphne","text":"The solution provided in this package is based on scicatlive which rather addresses the needs of software developers. We have worked on it to adopt it to our needs. The main difference are: use of a persistent mongodb in the backend configurable frontend using prebuild docker images integration of additional services","title":"scicatlive vs. scicat4daphne"},{"location":"access_managment/","text":"Authentication \ud83d\udd36 One idea is to look into ORCID as identity provider. \ud83d\udd34 Still to come... Access management \ud83d\udd34 Still to come...","title":"Access managment"},{"location":"access_managment/#authentication","text":"\ud83d\udd36 One idea is to look into ORCID as identity provider. \ud83d\udd34 Still to come...","title":"Authentication"},{"location":"access_managment/#access-management","text":"\ud83d\udd34 Still to come...","title":"Access management"},{"location":"further_ideas/","text":"List of potential further ideas that still have to be discussed: NeXus / HDF5 data viewer that can access datasets listed in SciCat a bit of doc how to get started with PySciCat","title":"Further ideas"},{"location":"further_ideas/#list-of-potential-further-ideas-that-still-have-to-be-discussed","text":"NeXus / HDF5 data viewer that can access datasets listed in SciCat a bit of doc how to get started with PySciCat","title":"List of potential further ideas that still have to be discussed:"},{"location":"ingestion_frontend/","text":"Project is still under active development Please consider this project in it's current state just as a draft. The api is not yet stable and there may still be important security vulnerabilities. Documentation is still far from being complete. Upload / ingestion frontend With this additional service we aim to provide the possibility create Datasets with structured Metadata through a web frontend.","title":"Ingestion frontend"},{"location":"ingestion_frontend/#upload-ingestion-frontend","text":"With this additional service we aim to provide the possibility create Datasets with structured Metadata through a web frontend.","title":"Upload / ingestion frontend"},{"location":"installation/","text":"Project is still under active development Please consider this project in it's current state just as a draft. The api is not yet stable and there may still be important security vulnerabilities. Documentation is still far from being complete. Installation of SciCat4daphne Requirements The described installation has been tested on the following systems: Virtual machine running Ubuntu 20.04 Tested dependency Versions: docker version \ud83d\udd34 docker-compose version \ud83d\udd34 installed via \ud83d\udd34 Getting started Just navigate into the scicatlive4daphne directory and run docker-compose up -d to look at log messages use docker-compose logs to stop all services use docker-compose down Necessary adoptions \ud83d\udd34 Still to be specified. config of upload frontend password of MongoExpress passwords of default users Services in detail This is an overview of the different services invoked through docker-compose together with the most important configuration options. ask questions / contribute to the configuration Do not hesitate to participate in GitHub Issue \ud83d\udd34 SciCat Backend (Catamel) the configuration file is located in scicatlive4daphne/config/catamel/config.local.js but should not need site-specific changes for a basic installation. For a production environment the entries pidPrefix and site should be specified. SciCat Frontend (Catanie) this service uses the configuration file scicatlive4daphne/config/catanie/config.json where e.g. the appearance of the scientific metadata (tree view vs. simple tabular view) and the columns of the dataset table can be configured. Reverse Proxy (Traefik) this service is used to stitch the different services in docker-compose together Database (MongoDB) all configuration happens in the docker-compose.yaml . Most important is the mounting point for the _ mongoDB_ on the host system which is specified at volumes: - \"/srv/mongodb:/bitnami/mongodb\" # check that is mount exists on local file-system fd Obvious TODOs: - specify the user of the host system that creates the db (currently it is the user with the uid \ud83d\udd34) Upload Frontend Under development This service may still change significantly especially if parts of it will be ported to Catanie. Please consider this part of the project as a working draft to to define the necessary features. Can be found at http://scicat-host/upload . Addon API Currently there is no configuration for this service. An interactive api-documentation can be found at http://scicat-host/swagger-ui . Not Stable This service may disappear completely in case it's functionality would be integrated into Catamel. Know issues: calls to this additional backend api currently do not check the token issued by Catamel MongoDB Web GUI (MongoExpress) Mainly intended for those who are managing the installation and for inspection of the DB. By default this service is deactivated (commented out in docker-compose.yaml ). The login for this service is specified in the docker-compose.yaml and managed by traefik . The password is hashed using \ud83d\udd34 which can e.g. be generated at \ud83d\udd34. Maintenance and House Keeping Backup The current backup strategy \ud83d\udd34 \ud83d\udd36 TODO: Once there is a script for backup describe it here Restore from Backup \ud83d\udd34 Still to come... search-api \ud83d\udd36 TODO: still to be seen if there is a sensible how to integrate this in this context.","title":"Installation"},{"location":"installation/#installation-of-scicat4daphne","text":"","title":"Installation of SciCat4daphne"},{"location":"installation/#requirements","text":"The described installation has been tested on the following systems: Virtual machine running Ubuntu 20.04 Tested dependency Versions: docker version \ud83d\udd34 docker-compose version \ud83d\udd34 installed via \ud83d\udd34","title":"Requirements"},{"location":"installation/#getting-started","text":"Just navigate into the scicatlive4daphne directory and run docker-compose up -d to look at log messages use docker-compose logs to stop all services use docker-compose down","title":"Getting started"},{"location":"installation/#necessary-adoptions","text":"\ud83d\udd34 Still to be specified. config of upload frontend password of MongoExpress passwords of default users","title":"Necessary adoptions"},{"location":"installation/#services-in-detail","text":"This is an overview of the different services invoked through docker-compose together with the most important configuration options. ask questions / contribute to the configuration Do not hesitate to participate in GitHub Issue \ud83d\udd34","title":"Services in detail"},{"location":"installation/#scicat-backend-catamel","text":"the configuration file is located in scicatlive4daphne/config/catamel/config.local.js but should not need site-specific changes for a basic installation. For a production environment the entries pidPrefix and site should be specified.","title":"SciCat Backend (Catamel)"},{"location":"installation/#scicat-frontend-catanie","text":"this service uses the configuration file scicatlive4daphne/config/catanie/config.json where e.g. the appearance of the scientific metadata (tree view vs. simple tabular view) and the columns of the dataset table can be configured.","title":"SciCat Frontend (Catanie)"},{"location":"installation/#reverse-proxy-traefik","text":"this service is used to stitch the different services in docker-compose together","title":"Reverse Proxy (Traefik)"},{"location":"installation/#database-mongodb","text":"all configuration happens in the docker-compose.yaml . Most important is the mounting point for the _ mongoDB_ on the host system which is specified at volumes: - \"/srv/mongodb:/bitnami/mongodb\" # check that is mount exists on local file-system fd Obvious TODOs: - specify the user of the host system that creates the db (currently it is the user with the uid \ud83d\udd34)","title":"Database (MongoDB)"},{"location":"installation/#upload-frontend","text":"Under development This service may still change significantly especially if parts of it will be ported to Catanie. Please consider this part of the project as a working draft to to define the necessary features. Can be found at http://scicat-host/upload .","title":"Upload Frontend"},{"location":"installation/#addon-api","text":"Currently there is no configuration for this service. An interactive api-documentation can be found at http://scicat-host/swagger-ui . Not Stable This service may disappear completely in case it's functionality would be integrated into Catamel. Know issues: calls to this additional backend api currently do not check the token issued by Catamel","title":"Addon API"},{"location":"installation/#mongodb-web-gui-mongoexpress","text":"Mainly intended for those who are managing the installation and for inspection of the DB. By default this service is deactivated (commented out in docker-compose.yaml ). The login for this service is specified in the docker-compose.yaml and managed by traefik . The password is hashed using \ud83d\udd34 which can e.g. be generated at \ud83d\udd34.","title":"MongoDB Web GUI (MongoExpress)"},{"location":"installation/#maintenance-and-house-keeping","text":"","title":"Maintenance and House Keeping"},{"location":"installation/#backup","text":"The current backup strategy \ud83d\udd34 \ud83d\udd36 TODO: Once there is a script for backup describe it here","title":"Backup"},{"location":"installation/#restore-from-backup","text":"\ud83d\udd34 Still to come...","title":"Restore from Backup"},{"location":"installation/#search-api","text":"\ud83d\udd36 TODO: still to be seen if there is a sensible how to integrate this in this context.","title":"search-api"},{"location":"metadata_managment/","text":"Project is still under active development Please consider this project in it's current state just as a draft. The api is not yet stable and there may still be important security vulnerabilities. Documentation is still far from being complete. Metadata Schemas One of the unique features of SciCat is its flexibility in capturing Scientific Metadata. However, to assure a systematic (machine readable and AI-ready) structure of the metadata captured in SciCat one approach is to use well defined Metadata Schemas . Ideally these schemata are predefined and follow given ontologies (e.g. the PaNdata Ontology) or rules that are accepted in the community (e.g. NeXus). Unfortunately for many use cases it is very difficult to agree on a ridged metadata structure beforehand. Here we try to follow a more flexible approach that explicitly allows the creation and modification of metadata schemas any time. This is rather a pragmatic decision mainly due to the fact that for many experimental datasets it is not yet clear which metadata should be captured and to some extend it just postpones the decision on the final, ridged metadata structure. In a way this approach is inspired by what was developed in AMORE / AMARCORD (EuXFEL) . The SciCat Extention proposed here offers the following features: Metadata Schemas are stored in the mongoDB of SciCat Creation and Management of Metadata Schemas through the web frontend (see ingestion frontend) Metadata Schemas can be specific for \"Datasets\" or \"Samples\" Validation to check if of provided metadata follows the available metadata schemas \ud83d\udd36 Support of schemas with a managed catalog of entries Schema specification Data structure not stable Currently the schema definition is kept close to cerberus which may change in the future. Schema structure { \"schema_name\": \"measurement\", \"schema_type\": \"dataset\" \"fixed_value_entries\": false, \"keys\": [ { \"key_name\": \"measurement_type\", \"type\": \"string\", \"required\": true, \"unit\": null \"allowed\": [\"beamtime\", \"lab\"], \"changes_likely\": false, \"scan_ref\": false, } ], } The basic fields in the schema are schema_name and schema_type , where the type can either be dataset or sample . fixed_value_entries specifies \ud83d\udd36. For each key in the schema there are the following options: key_name : field name type : valid values are boolean , string , number , float , int , list required : specifies if a value for this field must be provided in order to pass validation unit : if there is a unit attached to this field it can be provided here allowed : can be used to specify a list of allowed values that will be changes_likely : highlights fields that are likely to change in case an exiting dataset is used as template for a new one. scan_ref : ... not yet used. Intended for validation in case of data references (might be removed in the future) schema : \ud83d\udd36 for lists etc. Examples of Metadata schemas \ud83d\udd36 still some text to come... Metadata schemas dedicated to a experimental techniques { \"schema_name\": \"GIWAXS\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": true, \"key_name\": \"sample_detector_distance\", \"required\": true, \"scan_ref\": false, \"type\": \"number\", \"unit\": \"mm\" }, { \"changes_likely\": true, \"key_name\": \"central_pixel\", \"required\": true, \"scan_ref\": false, \"schema\": { \"type\": \"number\" }, \"type\": \"list\", \"unit\": null } ] } Metadata schemas describing the dataset structure / experimental procedures { \"schema_name\": \"logbook\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": false, \"key_name\": \"logbook_file\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"logbook_pages\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] } Managed schemas \ud83d\udd36 still to be implemented in frontend... \ud83d\udd36 The idea here is to have not only the schema in the DB but also a list of know entries to these schemas that can be can be used as metadata. \ud83d\udd36 Needs more detailed explanation. \ud83d\udd36 this is what the fixed_value_entries key refers to. Example: Material database for samples One use-case for this could be a sort of DB which contains materials samples can be composed of. Here is an example for such a schema: { \"schema_name\": \"material\", \"schema_type\": \"sample\", \"multiples_entries\": true, \"fixed_value_entries\": true, \"id_key\": \"material_id\", \"keys\": [ { \"changes_likely\": true, \"key_name\": \"material_id\", \"required\": true, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"full_name\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"formula\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] } and some entries in corresponding database collections would be: { material_id: 'PEN', full_name: 'Pentacene', formula: 'C22H14' }, { material_id: 'Au', full_name: 'gold' }","title":"Metadata managment"},{"location":"metadata_managment/#metadata-schemas","text":"One of the unique features of SciCat is its flexibility in capturing Scientific Metadata. However, to assure a systematic (machine readable and AI-ready) structure of the metadata captured in SciCat one approach is to use well defined Metadata Schemas . Ideally these schemata are predefined and follow given ontologies (e.g. the PaNdata Ontology) or rules that are accepted in the community (e.g. NeXus). Unfortunately for many use cases it is very difficult to agree on a ridged metadata structure beforehand. Here we try to follow a more flexible approach that explicitly allows the creation and modification of metadata schemas any time. This is rather a pragmatic decision mainly due to the fact that for many experimental datasets it is not yet clear which metadata should be captured and to some extend it just postpones the decision on the final, ridged metadata structure. In a way this approach is inspired by what was developed in AMORE / AMARCORD (EuXFEL) . The SciCat Extention proposed here offers the following features: Metadata Schemas are stored in the mongoDB of SciCat Creation and Management of Metadata Schemas through the web frontend (see ingestion frontend) Metadata Schemas can be specific for \"Datasets\" or \"Samples\" Validation to check if of provided metadata follows the available metadata schemas \ud83d\udd36 Support of schemas with a managed catalog of entries","title":"Metadata Schemas"},{"location":"metadata_managment/#schema-specification","text":"Data structure not stable Currently the schema definition is kept close to cerberus which may change in the future.","title":"Schema specification"},{"location":"metadata_managment/#schema-structure","text":"{ \"schema_name\": \"measurement\", \"schema_type\": \"dataset\" \"fixed_value_entries\": false, \"keys\": [ { \"key_name\": \"measurement_type\", \"type\": \"string\", \"required\": true, \"unit\": null \"allowed\": [\"beamtime\", \"lab\"], \"changes_likely\": false, \"scan_ref\": false, } ], } The basic fields in the schema are schema_name and schema_type , where the type can either be dataset or sample . fixed_value_entries specifies \ud83d\udd36. For each key in the schema there are the following options: key_name : field name type : valid values are boolean , string , number , float , int , list required : specifies if a value for this field must be provided in order to pass validation unit : if there is a unit attached to this field it can be provided here allowed : can be used to specify a list of allowed values that will be changes_likely : highlights fields that are likely to change in case an exiting dataset is used as template for a new one. scan_ref : ... not yet used. Intended for validation in case of data references (might be removed in the future) schema : \ud83d\udd36 for lists etc.","title":"Schema structure"},{"location":"metadata_managment/#examples-of-metadata-schemas","text":"\ud83d\udd36 still some text to come...","title":"Examples of Metadata schemas"},{"location":"metadata_managment/#metadata-schemas-dedicated-to-a-experimental-techniques","text":"{ \"schema_name\": \"GIWAXS\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": true, \"key_name\": \"sample_detector_distance\", \"required\": true, \"scan_ref\": false, \"type\": \"number\", \"unit\": \"mm\" }, { \"changes_likely\": true, \"key_name\": \"central_pixel\", \"required\": true, \"scan_ref\": false, \"schema\": { \"type\": \"number\" }, \"type\": \"list\", \"unit\": null } ] }","title":"Metadata schemas dedicated to a experimental techniques"},{"location":"metadata_managment/#metadata-schemas-describing-the-dataset-structure-experimental-procedures","text":"{ \"schema_name\": \"logbook\", \"schema_type\": \"dataset\", \"fixed_value_entries\": false, \"keys\": [ { \"changes_likely\": false, \"key_name\": \"logbook_file\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"logbook_pages\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] }","title":"Metadata schemas describing the dataset structure / experimental procedures"},{"location":"metadata_managment/#managed-schemas","text":"\ud83d\udd36 still to be implemented in frontend... \ud83d\udd36 The idea here is to have not only the schema in the DB but also a list of know entries to these schemas that can be can be used as metadata. \ud83d\udd36 Needs more detailed explanation. \ud83d\udd36 this is what the fixed_value_entries key refers to.","title":"Managed schemas"},{"location":"metadata_managment/#example-material-database-for-samples","text":"One use-case for this could be a sort of DB which contains materials samples can be composed of. Here is an example for such a schema: { \"schema_name\": \"material\", \"schema_type\": \"sample\", \"multiples_entries\": true, \"fixed_value_entries\": true, \"id_key\": \"material_id\", \"keys\": [ { \"changes_likely\": true, \"key_name\": \"material_id\", \"required\": true, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"full_name\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null }, { \"changes_likely\": true, \"key_name\": \"formula\", \"required\": false, \"scan_ref\": false, \"type\": \"string\", \"unit\": null } ] } and some entries in corresponding database collections would be: { material_id: 'PEN', full_name: 'Pentacene', formula: 'C22H14' }, { material_id: 'Au', full_name: 'gold' }","title":"Example: Material database for samples"}]}